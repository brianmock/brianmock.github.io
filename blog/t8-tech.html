<!DOCTYPE html>
<html>
<head>
  <title>DBC Week 8 Tech Blog Post</title>
  <meta charset="UTF-8">
  <link rel="stylesheet" type="text/css" href="default.css">
</head>
<body>
<main>
  <header>
  <h1>Memory and its Implications</h1>
  </header>
  <h2>Getting Close to the Metal</h2>
  <h4>August 31, 2015</h4>

  <article>
    <p>
      I've always been fascinated by how when you get down to the basics of computing, it is really just electrical signals being fired in a highly choreographed manner. Everything we see on a display is really the result of thousands (hundreds of thousands? millions?) little charges that are interpretted by a chip. This is at least what I have been thinking computing was at its basic level for the past 4-5 years since I started working in the tech industry. It's time to get down to the nuts and bolts and try to understand what is really going on. As they say in "Halt and Catch Fire" and probably elsewhere, let's get close to the metal.
    </p>
    <p>
      The purpose of memory is to store a signal that can be then accessed at another time unchanged. Reliable ways to store these signals have taken many forms over the years. Initially, these signals were stored in vacuum tubes that would contain octal-based radio waves. These radio waves could exist in these vacuum tubes and be referenced over and over or manipulated if needed. Next came glass tubes filled with mercury and capped with quartz crystals. These tubes could contain sound waves with the quartz crystals being the transducer that could interpret the waves. The problem with these forms was the fact that if power was cut, they would no longer hold their signals (i.e. information would be lost). The development of non-volatile memory was pioneered by the trio, Jay Forrester, Jan A. Rajchman and An Wang. They came up with magnetic core memory that would retain its information after power loss. The final step has been the switch to transistor based memory that has been the leading technology to the present day.
    </p>
    <p>
      As a programmer, we don't care as much about what is doing the storing of memory but rather how this memory is managed by our programs. To quote wikipedia, <a href="https://en.wikipedia.org/wiki/Memory_management">"The essential requirement of memory management is to provide ways to dynamically allocate portions of memory to programs at their request, and free it for reuse when no longer needed."</a> Any developer knows that if their program is writing memory outside of where it is supposed to, you are going to end up with problems. Defining what areas are accessible and which are out-of-bounds is key to a successful program. Generally, a program needs to check if the amount of memory available will meet the requirements of the program before proceeding. If there is enough space, grab that and do your thing with it. Once you are done, it would be a tragedy to just leave it there. You would basically be using permanent marker on the piece of memory and it wouldn't be open for use by other programs. You'll want to return the memory back to the heap of free memory so that other programs may go about their business. Once you forget to return the memory, it is a bit of a runaway process where everytime you run the program, you are losing a bit of memory that does not get returned and you will eventuall run out of space. So don't do that.
    </p>

      <!-- copy and paste as many sections as you want to add paragraphs -->
  </article>
</main>
</body>
</html>